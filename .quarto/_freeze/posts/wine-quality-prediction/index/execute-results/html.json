{
  "hash": "1546b058665ce8f41649cb1fb5602d06",
  "result": {
    "markdown": "---\ntitle: \"Wine Quality Prediction\"\ndescription:  \"On June 14th, 2018, I participated in a Data Hackathon as part of the REU program at George Mason University.\"\nauthor: \"Howard Baek\"\ndate: \"2018-06-18\"\ncategories: [Research]\nimage: \"thumbnail.jpg\"\n---\n\n\n## Introduction\n\nOn June 14th, 2018, I participated in a Data Hackathon as part of the REU program at George Mason University. It was my first ever hackathon and I was excited to finally participate in one. It lasted approximately 4 hours, from 10am to 2pm. Our team, consisting of three undergraduate students, worked with the famous Wine Quality dataset, which is hosted by [University of California Irvine's Center for Machine Learning and Intelligent Systems](https://archive.ics.uci.edu/ml/index.php). The goal of the hackathon was to accurately predict the Quality variable (\"Good\"= 1 or \"Bad\" = 0)\n\n## Dataset\n\nI first import the dataset and observe it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load tidyverse and caret package\nlibrary(tidyverse)\nlibrary(caret)\n\n# Import training / test data\nwine_train <- read_csv(\"wine_train.csv\")\nwine_test <- read_csv(\"wine_test.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(wine_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 799\nColumns: 12\n$ `fixed acidity`        <dbl> 7.4, 7.8, 7.8, 11.2, 7.4, 7.4, 7.9, 7.3, 7.8, 7…\n$ `volatile acidity`     <dbl> 0.700, 0.880, 0.760, 0.280, 0.700, 0.660, 0.600…\n$ `citric acid`          <dbl> 0.00, 0.00, 0.04, 0.56, 0.00, 0.00, 0.06, 0.00,…\n$ `residual sugar`       <dbl> 1.9, 2.6, 2.3, 1.9, 1.9, 1.8, 1.6, 1.2, 2.0, 6.…\n$ chlorides              <dbl> 0.076, 0.098, 0.092, 0.075, 0.076, 0.075, 0.069…\n$ `free sulfur dioxide`  <dbl> 11, 25, 15, 17, 11, 13, 15, 15, 9, 17, 15, 17, …\n$ `total sulfur dioxide` <dbl> 34, 67, 54, 60, 34, 40, 59, 21, 18, 102, 65, 10…\n$ density                <dbl> 0.9978, 0.9968, 0.9970, 0.9980, 0.9978, 0.9978,…\n$ pH                     <dbl> 3.51, 3.20, 3.26, 3.16, 3.51, 3.51, 3.30, 3.39,…\n$ sulphates              <dbl> 0.56, 0.68, 0.65, 0.58, 0.56, 0.56, 0.46, 0.47,…\n$ alcohol                <dbl> 9.4, 9.8, 9.8, 9.8, 9.4, 9.4, 9.4, 10.0, 9.5, 1…\n$ Quality                <chr> \"B\", \"B\", \"B\", \"G\", \"B\", \"B\", \"B\", \"G\", \"G\", \"B…\n```\n:::\n\n```{.r .cell-code}\nglimpse(wine_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 800\nColumns: 11\n$ `fixed acidity`        <dbl> 9.4, 7.2, 8.6, 5.1, 7.7, 8.4, 8.2, 8.4, 8.2, 7.…\n$ `volatile acidity`     <dbl> 0.500, 0.610, 0.550, 0.585, 0.560, 0.520, 0.280…\n$ `citric acid`          <dbl> 0.34, 0.08, 0.09, 0.00, 0.08, 0.22, 0.40, 0.39,…\n$ `residual sugar`       <dbl> 3.60, 4.00, 3.30, 1.70, 2.50, 2.70, 2.40, 2.00,…\n$ chlorides              <dbl> 0.082, 0.082, 0.068, 0.044, 0.114, 0.084, 0.052…\n$ `free sulfur dioxide`  <dbl> 5, 26, 8, 14, 14, 4, 4, 4, 4, 4, 4, 4, 7, 20, 4…\n$ `total sulfur dioxide` <dbl> 14, 108, 17, 86, 46, 18, 10, 10, 10, 12, 15, 14…\n$ density                <dbl> 0.99870, 0.99641, 0.99735, 0.99264, 0.99710, 0.…\n$ pH                     <dbl> 3.29, 3.25, 3.23, 3.56, 3.24, 3.26, 3.33, 3.27,…\n$ sulphates              <dbl> 0.52, 0.51, 0.44, 0.94, 0.66, 0.57, 0.70, 0.71,…\n$ alcohol                <dbl> 10.7, 9.4, 10.0, 12.9, 9.6, 9.9, 12.8, 12.5, 12…\n```\n:::\n:::\n\n\nTraining data has 799 observations and 12 variables, including the target variable, Quality, while the testing data has 800 observations and exactly the same attributes except Quality.\n\n## Data Manipulation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Change columns names- Take out single quotations and underscores from names \nnames(wine_train) <- gsub(\"'\", '', names(wine_train))\nnames(wine_train) <- gsub(\" \", \"_\", names(wine_train))\nnames(wine_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"fixed_acidity\"        \"volatile_acidity\"     \"citric_acid\"         \n [4] \"residual_sugar\"       \"chlorides\"            \"free_sulfur_dioxide\" \n [7] \"total_sulfur_dioxide\" \"density\"              \"pH\"                  \n[10] \"sulphates\"            \"alcohol\"              \"Quality\"             \n```\n:::\n\n```{.r .cell-code}\nnames(wine_test) <- gsub(\"'\", '', names(wine_test))\nnames(wine_test) <- gsub(\" \", \"_\", names(wine_test))\nnames(wine_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"fixed_acidity\"        \"volatile_acidity\"     \"citric_acid\"         \n [4] \"residual_sugar\"       \"chlorides\"            \"free_sulfur_dioxide\" \n [7] \"total_sulfur_dioxide\" \"density\"              \"pH\"                  \n[10] \"sulphates\"            \"alcohol\"             \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Change values in Quality column: \"B\" = 0 & \"G\" = 1\nwine_train <- wine_train %>% \n  mutate(Quality = ifelse(Quality == \"B\", 0, 1))\n\n# Observe number of 0s and 1s\ntable(wine_train$Quality)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  0   1 \n425 374 \n```\n:::\n:::\n\n\n<br>\n\n## Feature Selection\n\nI first wanted to select the relevant and useful features by means of feature selection in the [caret package](https://topepo.github.io/caret/index.html), a popular R package for statistical machine learning. This tutorial got me started: https://machinelearningmastery.com/feature-selection-with-the-caret-r-package/\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Feature Selection #1\nset.seed(7) # Bring me luck\ntrain_cor <- cor(wine_train[, -length(names(wine_train))])\n\n# summarize the correlation matrix\nprint(train_cor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                     fixed_acidity volatile_acidity citric_acid residual_sugar\nfixed_acidity         1.0000000000      -0.30241919  0.69369727     0.17320660\nvolatile_acidity     -0.3024191923       1.00000000 -0.54708405    -0.03046016\ncitric_acid           0.6936972747      -0.54708405  1.00000000     0.13322604\nresidual_sugar        0.1732066025      -0.03046016  0.13322604     1.00000000\nchlorides            -0.0003612805      -0.01426396  0.19704211    -0.02939710\nfree_sulfur_dioxide  -0.1562296716       0.03287532 -0.04284979     0.17117834\ntotal_sulfur_dioxide -0.2105196234       0.08831363 -0.01356392     0.14129767\ndensity               0.7293551024      -0.13418708  0.44405907     0.39166663\npH                   -0.6865541686       0.26154512 -0.56343133    -0.05858156\nsulphates             0.1618399522      -0.26807580  0.28944979     0.02657419\nalcohol               0.1261203673      -0.08160458  0.18895405     0.19244357\n                         chlorides free_sulfur_dioxide total_sulfur_dioxide\nfixed_acidity        -0.0003612805        -0.156229672         -0.210519623\nvolatile_acidity     -0.0142639627         0.032875319          0.088313629\ncitric_acid           0.1970421057        -0.042849793         -0.013563916\nresidual_sugar       -0.0293970980         0.171178339          0.141297672\nchlorides             1.0000000000         0.001938843          0.022849048\nfree_sulfur_dioxide   0.0019388434         1.000000000          0.730655240\ntotal_sulfur_dioxide  0.0228490477         0.730655240          1.000000000\ndensity               0.0754952020        -0.033797520         -0.085640019\npH                   -0.2466119148         0.086088169          0.009654689\nsulphates             0.4123789331         0.050006477          0.054326009\nalcohol              -0.1500365030        -0.019738532         -0.112954461\n                         density           pH   sulphates     alcohol\nfixed_acidity         0.72935510 -0.686554169  0.16183995  0.12612037\nvolatile_acidity     -0.13418708  0.261545116 -0.26807580 -0.08160458\ncitric_acid           0.44405907 -0.563431327  0.28944979  0.18895405\nresidual_sugar        0.39166663 -0.058581563  0.02657419  0.19244357\nchlorides             0.07549520 -0.246611915  0.41237893 -0.15003650\nfree_sulfur_dioxide  -0.03379752  0.086088169  0.05000648 -0.01973853\ntotal_sulfur_dioxide -0.08564002  0.009654689  0.05432601 -0.11295446\ndensity               1.00000000 -0.379103361  0.13497642 -0.18361374\npH                   -0.37910336  1.000000000 -0.28483760  0.12470433\nsulphates             0.13497642 -0.284837597  1.00000000  0.09629489\nalcohol              -0.18361374  0.124704335  0.09629489  1.00000000\n```\n:::\n\n```{.r .cell-code}\n# find attributes that are highly corrected (ideally >0.75)\nhigh_cor <- findCorrelation(train_cor, cutoff=0.5)\n\n# print indexes of highly correlated attributes\nprint(high_cor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1 3 7\n```\n:::\n:::\n\n\n-   Index 1 = `fixed_acidity`\n-   Index 2 = `citric_acid`\n-   Index 3 = `total_sulfur_dioxide`\n\n## Model Fitting\n\nSince `fixed_acidity`, `citric_acid` and `total_sulfur_dioxide` are highly correlated (redundant), I only used one of these features (`total_sulfur_dioxide`) and disposed of the two redundant ones (`fixed_acidity`, `citric_acid`). At this point, I formulated a hypothesis: a model without redundant features performs better than a model with redundant features. Let's find out if this is true.\n\n*Since the target variable is binary, I fit a logistic regression.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Logistic Regression\nwine_train$Quality <- factor(wine_train$Quality, levels = c(0, 1))\ntrain_log <- createDataPartition(wine_train$Quality, p=0.6, list=FALSE)\ntraining <- wine_train[train_log, ]\ntesting <- wine_train[ -train_log, ]\n\n# Hypothesis: Try logistic regression with all the predictor variables\nmod_log <- train(Quality ~ .,  data=training, method=\"glm\", family=\"binomial\")\n\nexp(coef(mod_log$finalModel))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         (Intercept)        fixed_acidity     volatile_acidity \n        5.228352e+52         1.362305e+00         4.014862e-02 \n         citric_acid       residual_sugar            chlorides \n        1.273078e-01         1.032885e+00         2.292574e-01 \n free_sulfur_dioxide total_sulfur_dioxide              density \n        1.047235e+00         9.684034e-01         1.807645e-58 \n                  pH            sulphates              alcohol \n        1.667995e+00         1.355519e+01         2.251100e+00 \n```\n:::\n\n```{.r .cell-code}\npred <- predict(mod_log, newdata=testing)\naccuracy <- table(pred, testing$Quality)\nsum(diag(accuracy))/sum(accuracy)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7147335\n```\n:::\n\n```{.r .cell-code}\n# Hypothesis: Try logistic regression without the redundant variables\n# Try logistic regression without highly correlated variables\nmod_log_2 <- train(Quality ~ volatile_acidity + residual_sugar + chlorides + free_sulfur_dioxide + total_sulfur_dioxide + density + pH + sulphates + alcohol,  data=training, method=\"glm\", family=\"binomial\")\n\npred_2 <- predict(mod_log_2, newdata = testing)\naccuracy_2 <- table(pred_2, testing$Quality)\nsum(diag(accuracy_2))/sum(accuracy_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7053292\n```\n:::\n:::\n\n\nThe first hypothesis yields an accuracy rate of 71.5%! while the first hypothesis yields 70.8%! Apparently, including all the variables yields higher accuracy.\n\n## Cross Validation\n\nAt this point, I looked on the tutorial page for the caret package to learn how to cross validate. I learned about `trainControl`, a function \"used to specify the type of resampling\". The parameter, `method`, specifies `repeatedcv`, which stands for repeated cross validation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cross validation on the second model where I took out the redundant variables\nctrl <- trainControl(method = \"repeatedcv\", repeats = 10)\n\n# Train logistic regression model\nmod_log_2_ctrl <- train(Quality ~ volatile_acidity + residual_sugar + chlorides + free_sulfur_dioxide +\n                     total_sulfur_dioxide + density + pH + sulphates + alcohol, data=training, \n                     trControl = ctrl, method=\"glm\", family=\"binomial\")\npred_2_ctrl <- predict(mod_log_2, newdata = testing)\naccuracy_2_ctrl <- table(pred_2, testing$Quality)\nsum(diag(accuracy_2_ctrl))/sum(accuracy_2_ctrl)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7053292\n```\n:::\n:::\n\n\nI got the same accuracy, which means that I didn't use cross validation properly...I'll have to learn more.\n\n## Advanced Models\n\nIn an effort to achieve a higher accuracy score, I looked for more accurate and powerful models, such as XgBoost, Random Forests, etc.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# XgBoost -----------------------------------------------------------------\nmod_xgboost <- train(Quality ~ ., data=training, \n      trControl = ctrl, method=\"xgbTree\", family=\"binomial\")\npred_xgboost <- predict(mod_xgboost, newdata = testing)\nacc_xgboost <- table(pred_xgboost, testing$Quality)\nsum(diag(acc_xgboost))/sum(acc_xgboost)\n# 70.8%\n\n# Random Forest-----------------------------------------------------------------\nmod_rf <- train(Quality ~ volatile_acidity + residual_sugar + chlorides + free_sulfur_dioxide +\n                  total_sulfur_dioxide + density + pH + sulphates + alcohol, data=training, \n                trControl = ctrl, method=\"rf\", family=\"binomial\")\npred_rf <- predict(mod_rf, newdata = testing)\nacc_rf <- table(pred_rf, testing$Quality)\nsum(diag(acc_rf)) / sum(acc_rf)\n# 80.6% is an improvement!\n\n# Logit Boost-----------------------------------------------------------------\nmod_logit <- train(Quality ~ volatile_acidity + residual_sugar + chlorides + free_sulfur_dioxide +\n                  total_sulfur_dioxide + density + pH + sulphates + alcohol, data=training, \n                trControl = ctrl, method=\"LogitBoost\")\npred_logit <- predict(mod_logit, newdata = testing)\nacc_logit <- table(pred_logit, testing$Quality)\nsum(diag(acc_logit)) / sum(acc_logit)\n# 72.1%\n\n# svmRadial-----------------------------------------------------------------\nmod_svm <- train(Quality ~ volatile_acidity + residual_sugar + chlorides + free_sulfur_dioxide +\n                     total_sulfur_dioxide + density + pH + sulphates + alcohol, data=training, \n                   trControl = ctrl, method=\"svmRadial\")\npred_svm <- predict(mod_svm, newdata = testing)\nacc_svm <- table(pred_svm, testing$Quality)\nsum(diag(acc_svm)) / sum(acc_svm)\n# 75.2%\n\n# LMT-----------------------------------------------------------------\nmod_svm_linear <- train(Quality ~ volatile_acidity + residual_sugar + chlorides + free_sulfur_dioxide +\n                   total_sulfur_dioxide + density + pH + sulphates + alcohol, data=training, \n                 trControl = ctrl, method=\"svmLinearWeights2\")\npred_svm_linear <- predict(mod_svm_linear, newdata = testing)\nacc_svm_linear <- table(pred_svm_linear, testing$Quality)\nsum(diag(acc_svm_linear)) / sum(acc_svm_linear)\n```\n:::\n\n\n## Conclusion\n\nI learned about a totally new field in machine learning. Importantly, is very interesting and motivating since coming up with machine learning models feels like creating and refining a crystal ball that shows the future. In the future, I plan on reading through [this comprehensive tutorial of the caret package](https://topepo.github.io/caret/index.html), take machine learning courses on [DataCamp](www.datacamp.com), and hope to learn from the mistakes I made during this hackathon.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}